{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAQDjVcychvD"
      },
      "source": [
        "# LLM setup notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKmSJcItYiN_"
      },
      "outputs": [],
      "source": [
        "!pip install ollama -q > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5qSqQC4_5pO"
      },
      "outputs": [],
      "source": [
        "# Importing all the necessary libraries\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import os\n",
        "import json\n",
        "import ollama\n",
        "import re\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNJUMloFH-F1",
        "outputId": "82556207-0822-4791-a7bb-107e6d1dea0e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlCasd2hVlzj"
      },
      "source": [
        "### Code to connect all the files in within the ZIP file into one DF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZEVbTPd8XTT"
      },
      "outputs": [],
      "source": [
        "# Path to the ZIP file\n",
        "zip_file_path = '/content/drive/My Drive/Files/file.zip'\n",
        "\n",
        "# Temporary folder to extract JSON files\n",
        "extract_folder = '/content/json_files'\n",
        "\n",
        "# Extract all files from the ZIP\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_folder)\n",
        "\n",
        "# Initialize a list to store data\n",
        "dataframes = []\n",
        "\n",
        "# Iterate over each JSON file in the extracted folder\n",
        "for file_name in os.listdir(extract_folder):\n",
        "    if file_name.endswith('.json'):  # Check if it's a JSON file\n",
        "        file_path = os.path.join(extract_folder, file_name)\n",
        "        try:\n",
        "            # Read the JSON file\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                data = json.load(file)\n",
        "\n",
        "                df_temp = pd.DataFrame(data)\n",
        "                dataframes.append(df_temp)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {file_name}: {e}\")\n",
        "\n",
        "# Combine all DataFrames into one named df\n",
        "df = pd.concat(dataframes, ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p0iHNVGAbnE"
      },
      "source": [
        "## Data Preprocessing Before LLM\n",
        "\n",
        "In this step, we preprocess the data to ensure it is ready for the LLM. We remove rows with missing or insufficient information, as well as those that do not align with the project's objectives, ensuring that only relevant and complete data is passed for further processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gl3KRBoBZkym",
        "outputId": "690d70b5-570b-4468-f651-65718b2f733d"
      },
      "outputs": [],
      "source": [
        " # Calculate word counts for each row in the 'HTML_Text' column\n",
        "df['word_count'] = df['HTML_Text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Generate statistics on the word counts\n",
        "stats = df['word_count'].describe()\n",
        "\n",
        "# Print the statistics\n",
        "print(stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waeBgr8XUks7"
      },
      "source": [
        "### We remove the HTML texts where the word count is less than 400 as those do not give use enought info to work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xj5hc_OlZeQ0",
        "outputId": "edf89e54-4112-48b9-8694-5c728c6e9695"
      },
      "outputs": [],
      "source": [
        "# Define a threshold for minimum word count\n",
        "min_word_count = 400\n",
        "\n",
        "# Filter the original dataframe in place, only keeping rows meeting the word count requirement\n",
        "df = df[df['HTML_Text'].apply(lambda x: len(x.split()) > min_word_count)].copy()\n",
        "\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rYR74pdUqc7"
      },
      "source": [
        "### Removing additional letters from city names. For example ''Aalborg C'' would become ''Aalborg''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCzPFPiTHfbs"
      },
      "outputs": [],
      "source": [
        "# Function to city area indications\n",
        "def clean_trailing_indicators(area):\n",
        "    if pd.isna(area):\n",
        "        return None\n",
        "    # Remove things like 'o', 'c', 'SØ', 'N', 'K'\n",
        "    area = re.sub(r'\\s+[a-zA-ZÆØÅæøå]+$', '', area.strip())\n",
        "    return area\n",
        "\n",
        "# Apply the cleaning function to the 'Area' column\n",
        "df['Area'] = df['Area'].apply(clean_trailing_indicators)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MDcV2eRV2Ny"
      },
      "source": [
        "### Some ads have more than one city listed as an option. We only keep the city that was listed as the first option"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeD5lq1mJGAi"
      },
      "outputs": [],
      "source": [
        "# Simplified function to keep the first city\n",
        "def keep_first_place_simple(area):\n",
        "    if pd.isna(area):\n",
        "        return None\n",
        "    # Split by simpler delimiters\n",
        "    delimiters = [',', '/', ' og', ' eller', ' or', ' and']\n",
        "    for delim in delimiters:\n",
        "        if delim in area:\n",
        "            # Keep only the first part\n",
        "            area = area.split(delim)[0].strip()\n",
        "            break\n",
        "    # Return cleaned area if it is meaningful\n",
        "    return area if len(area) > 2 else None\n",
        "\n",
        "# Apply the simplified cleaning function\n",
        "df['Area'] = df['Area'].apply(keep_first_place_simple)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n29P_EAUx6y"
      },
      "source": [
        "### Mapping the 3 cities that have different name in English vs Danish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yNTLlHXRiP_"
      },
      "outputs": [],
      "source": [
        "# Mapping of Danish to English city names\n",
        "city_name_mapping = {\n",
        "    'København': 'Copenhagen',\n",
        "    'Århus': 'Aarhus',\n",
        "    'Helsingør': 'Elsinore'\n",
        "}\n",
        "\n",
        "# Replace Danish names with their English equivalents\n",
        "df['Area'] = df['Area'].replace(city_name_mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-a7IHUUOU6dT"
      },
      "source": [
        "### A list of cities we want to look at. If the area is not one of the ones listed, it is removed. These are the 30 biggest cities in Denmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBgfJQUyLoxW"
      },
      "outputs": [],
      "source": [
        "# List of 30 biggest cities in Denmark\n",
        "cities = [\n",
        "    \"Copenhagen\", \"Aarhus\", \"Odense\", \"Aalborg\", \"Esbjerg\", \"Randers\", \"Kolding\", \"Horsens\", \"Vejle\", \"Roskilde\",\n",
        "    \"Herning\", \"Hørsholm\", \"Silkeborg\", \"Næstved\", \"Fredericia\", \"Viborg\", \"Køge\", \"Holstebro\", \"Taastrup\", \"Slagelse\",\n",
        "    \"Hillerød\", \"Sønderborg\", \"Svendborg\", \"Hjørring\", \"Holbæk\", \"Frederikshavn\", \"Nørresundby\", \"Ringsted\", \"Haderslev\",\n",
        "    \"Skive\", \"Ølstykke-Stenløse\", \"Nykøbing Falster\", \"Greve Strand\", \"Kalundborg\", \"Ballerup\", \"Rødovre\", \"Lyngby\",\n",
        "    \"Albertslund\", \"Hvidovre\", \"Glostrup\", \"Ishøj\", \"Birkerød\", \"Farum\", \"Frederikssund\", \"Brøndby Strand\",\n",
        "    \"Skanderborg\", \"Hedensted\", \"Frederiksværk\", \"Lillerød\", \"Solrød Strand\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAwxWogOPBMM",
        "outputId": "6589a5ea-d203-480b-8b90-1e4862a94dcc"
      },
      "outputs": [],
      "source": [
        "# Filter the DataFrame to only include rows where Area is in the cities list\n",
        "df = df[df['Area'].isin(cities)]\n",
        "\n",
        "# Display the filtered DataFrame\n",
        "print(\"Filtered DataFrame:\")\n",
        "print(df.shape)\n",
        "\n",
        "# This is a sample data. Only for a few months"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0RvY3VtpLW4k",
        "outputId": "937fa79a-8992-4fd5-daaf-14962d757ff0"
      },
      "outputs": [],
      "source": [
        "# Install Ollama\n",
        "!sudo apt-get install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruGKfIYlMd2Q"
      },
      "outputs": [],
      "source": [
        "# Sets up environment variables and starts the Ollama server\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "\n",
        "def start_ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=start_ollama)\n",
        "ollama_thread.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSKKQZUxWxAU",
        "outputId": "19578382-61bf-4b98-b6fb-5b5bd28b1764"
      },
      "outputs": [],
      "source": [
        "!ollama run llama3.1:8b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaeOWSMugwt2"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are given a job listing. Your task is to extract relevant skills, categorizing them into **primary IT-related technical skills** and **secondary (soft/general) skills**.\n",
        "\n",
        "### Definitions:\n",
        "- **Primary Skills**: Technical, actionable skills directly relevant to IT roles, including tools, technologies, methods, or domain expertise.\n",
        "- **Secondary Skills**: Soft skills, behavioral traits, or general competencies that support workplace effectiveness.\n",
        "\n",
        "### Rules:\n",
        "1. Separate extracted skills into **primary** (IT-related technical) and **secondary** (soft/general) categories.\n",
        "2. Split compound terms into individual skills (e.g., \"Skill A and Skill B\" → [\"Skill A\", \"Skill B\"]).\n",
        "3. Exclude vague, repetitive, or non-actionable terms.\n",
        "4. Translate all extracted skills into **English**, regardless of the input language (e.g., Danish to English).\n",
        "5. Ensure the output is clear, specific, concise, and in valid JSON format.\n",
        "\n",
        "---\n",
        "\n",
        "### Output Format:\n",
        "```json\n",
        "{\n",
        "  \"skills\": {\n",
        "    \"primary\": [\"Skill 1\", \"Skill 2\", \"...\"],\n",
        "    \"secondary\": [\"Skill A\", \"Skill B\", \"...\"]\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### Examples:\n",
        "#### Example 1\n",
        "**Job Listing Text:** \"Looking for expertise in modern development practices and effective communication skills.\"\n",
        "\n",
        "**Output:**\n",
        "```json\n",
        "{\n",
        "  \"skills\": {\n",
        "    \"primary\": [\"Modern development practices\"],\n",
        "    \"secondary\": [\"Communication\"]\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "#### Example 2\n",
        "**Job Listing Text:** \"Vi søger en erfaren person med kendskab til ny teknologi og gode samarbejdsevner.\"\n",
        "\n",
        "**Output:**\n",
        "```json\n",
        "{\n",
        "  \"skills\": {\n",
        "    \"primary\": [\"New technology\"],\n",
        "    \"secondary\": [\"Collaboration\"]\n",
        "  }\n",
        "}\n",
        "```\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvOUzJVM9ded"
      },
      "outputs": [],
      "source": [
        "# Sets up environment variables and starts the Ollama server\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "\n",
        "def start_ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=start_ollama)\n",
        "ollama_thread.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GCB2JYi6-Zei",
        "outputId": "ed8739ec-242d-418e-e47c-9327d4513720"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# 1) Extract skills\n",
        "def extract_skills_from_text(html_text, row_id):\n",
        "    prompt = f\"\"\"\n",
        "    Extract the relevant skills from the following job listing, categorizing them into **primary (IT-related technical skills)** and **secondary (soft/general skills)**.\n",
        "\n",
        "    ### Rules:\n",
        "    - Focus on actionable, specific, and clearly stated skills.\n",
        "    - Outputs must be in **clear and concise English only**, regardless of the input language.\n",
        "    - If no skills are present, return empty lists for both categories.\n",
        "\n",
        "    Provide the results in this JSON format:\n",
        "    {{\n",
        "      \"skills\": {{\n",
        "        \"primary\": [\"Skill 1\", \"Skill 2\", \"...\"],\n",
        "        \"secondary\": [\"Skill A\", \"Skill B\", \"...\"]\n",
        "      }}\n",
        "    }}\n",
        "\n",
        "    Job Listing Text:\n",
        "    {html_text}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = ollama.generate(\n",
        "            model='llama3.1:8b',\n",
        "            prompt=prompt,\n",
        "            options={\"temperature\": 0.1}\n",
        "        )\n",
        "        response_content = response.get('response', '')\n",
        "\n",
        "        # Extract JSON block from the response\n",
        "        json_start = response_content.find('{')\n",
        "        json_end = response_content.rfind('}') + 1\n",
        "        if json_start == -1 or json_end == -1:\n",
        "            raise ValueError(\"JSON block not found in the response\")\n",
        "\n",
        "        # Parse extracted JSON\n",
        "        skills_data = json.loads(response_content[json_start:json_end])\n",
        "        return skills_data\n",
        "\n",
        "    except (ValueError, json.JSONDecodeError) as e:\n",
        "        print(f\"Error extracting skills for Row {row_id}: {e}\")\n",
        "        return {\"skills\": {\"primary\": [], \"secondary\": []}}\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error extracting skills for Row {row_id}: {e}\")\n",
        "        return {\"skills\": {\"primary\": [], \"secondary\": []}}\n",
        "\n",
        "# 2) Refine extracted skills\n",
        "def refine_skills_with_llm(skills_data, row_id):\n",
        "    # Prepare the prompt for refinement\n",
        "    prompt = f\"\"\"\n",
        "    Refine the following extracted skills to ensure they are:\n",
        "    - Deduplicated (remove synonyms/redundant entries).\n",
        "    - Correctly separated into **primary (IT/technical)** vs **secondary (soft/general)**.\n",
        "    - Outputs must be in **clear and concise English only**, regardless of the input language.\n",
        "\n",
        "\n",
        "    Input:\n",
        "    {json.dumps(skills_data, indent=2)}\n",
        "\n",
        "    Return the output in the same JSON structure:\n",
        "    {{\n",
        "      \"skills\": {{\n",
        "        \"primary\": [\"Refined Skill 1\", \"Refined Skill 2\", \"...\"],\n",
        "        \"secondary\": [\"Refined Skill A\", \"Refined Skill B\", \"...\"]\n",
        "      }}\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = ollama.generate(\n",
        "            model='llama3.1:8b',\n",
        "            prompt=prompt,\n",
        "            options={\"temperature\": 0.1}\n",
        "        )\n",
        "        response_content = response.get('response', '')\n",
        "\n",
        "        # Extract JSON block from the response\n",
        "        json_start = response_content.find('{')\n",
        "        json_end = response_content.rfind('}') + 1\n",
        "        if json_start == -1 or json_end == -1:\n",
        "            raise ValueError(\"JSON block not found in the response\")\n",
        "\n",
        "        # Parse refined JSON\n",
        "        refined_data = json.loads(response_content[json_start:json_end])\n",
        "        return refined_data\n",
        "\n",
        "    except (ValueError, json.JSONDecodeError) as e:\n",
        "        print(f\"Error refining skills for Row {row_id}: {e}\")\n",
        "        # Fall back to unrefined data\n",
        "        return skills_data\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error refining skills for Row {row_id}: {e}\")\n",
        "        # Fall back to unrefined data\n",
        "        return skills_data\n",
        "\n",
        "# 3) Process each row by extracting first, then refining\n",
        "def process_row(html_text, row_id):\n",
        "    extracted = extract_skills_from_text(html_text, row_id)\n",
        "    refined = refine_skills_with_llm(extracted, row_id)\n",
        "    print(f\"Row {row_id} - Extracted Skills: {extracted}\")\n",
        "    print(f\"Row {row_id} - Refined Skills: {refined}\")\n",
        "\n",
        "    return refined['skills']['primary'], refined['skills']['secondary']\n",
        "\n",
        "# Example usage with a DataFrame containing a column 'HTML_Text'\n",
        "df['Primary_Skills'], df['Secondary_Skills'] = zip(\n",
        "    *df['HTML_Text'].apply(lambda text: process_row(\n",
        "        text, df.index[df['HTML_Text'] == text].tolist()[0] + 1))\n",
        ")\n",
        "\n",
        "# Save final results\n",
        "output_file = 'updated_job_listings_with_refined_skills.csv'\n",
        "try:\n",
        "    df.to_csv(output_file, index=False, encoding='utf-8')\n",
        "    print(f\"Updated job listings saved to {output_file}.\")\n",
        "except IOError as e:\n",
        "    print(f\"Error saving file: {e}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
